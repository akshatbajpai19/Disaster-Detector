{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia8JdoVJWy22"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transform = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me4a-czRYCaY"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/data.zip'\n",
        "!mkdir train/all\n",
        "!mkdir train/all_paired\n",
        "!cp train/Flood/* train/all\n",
        "!cp train/Hurricane/* train/all\n",
        "!cp train/Earthquake/* train/all\n",
        "!cp train/Landslides/* train/all\n",
        "!cp train/Wildfire/* train/all\n",
        "!cp train/Earthquake_paired/* train/all_paired\n",
        "!cp train/Flood_paired/* train/all_paired\n",
        "!cp train/Hurricane_paired/* train/all_paired\n",
        "!cp train/Landslides_paired/* train/all_paired\n",
        "!cp train/Wildfire_paired/* train/all_paired\n",
        "!rm train/all/*.txt\n",
        "!rm train/all_paired/*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGWXsF_EY36K"
      },
      "outputs": [],
      "source": [
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, root_dir_original, root_dir_corrupted, transform=None):\n",
        "        self.root_dir_original = root_dir_original\n",
        "        self.root_dir_corrupted = root_dir_corrupted\n",
        "        self.transform = transform\n",
        "        self.original_image_list = os.listdir(root_dir_original)\n",
        "        self.corrupted_image_list = os.listdir(root_dir_corrupted)\n",
        "    def __len__(self):\n",
        "        return min(len(self.original_image_list), len(self.corrupted_image_list))\n",
        "    def __getitem__(self, idx):\n",
        "        original_img_name = self.original_image_list[idx]\n",
        "        corrupted_img_name = self.corrupted_image_list[idx]\n",
        "        original_img_path = os.path.join(self.root_dir_original, original_img_name)\n",
        "        corrupted_img_path = os.path.join(self.root_dir_corrupted, corrupted_img_name)\n",
        "        # Load the original and corrupted images\n",
        "        original_img = Image.open(original_img_path).convert('RGB')\n",
        "        corrupted_img = Image.open(corrupted_img_path).convert('RGB')\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            original_img = self.transform(original_img)\n",
        "            corrupted_img = self.transform(corrupted_img)\n",
        "        return original_img, corrupted_img, corrupted_img_name\n",
        "\n",
        "# Specify the path to your datasets\n",
        "original_dataset_root = '/content/train/all'\n",
        "corrupted_dataset_root = '/content/train/all_paired'\n",
        "# Create the dataset\n",
        "paired_image_dataset = PairedImageDataset(root_dir_original=original_dataset_root,\n",
        "                                          root_dir_corrupted=corrupted_dataset_root,\n",
        "                                          transform=transform)\n",
        "# Create the dataloader\n",
        "batch_size = 64\n",
        "dataloader = DataLoader(paired_image_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2zI8IFQZrqV"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self,num_channels=3,latent_dim=100):\n",
        "        super(Generator,self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.fc = nn.Linear(512 * 16 * 16, latent_dim)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 512 * 16 * 16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (512, 16, 16)),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ConvTranspose2d(64, num_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN-7O50CSD9S"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(num_channels * 2, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 32 * 32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rThGXrOWLYkL"
      },
      "outputs": [],
      "source": [
        "patch_size = 16\n",
        "class LocalDiscriminator(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_patches=4):\n",
        "        super(LocalDiscriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_channels * 2, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*16*16,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.fc = nn.Linear(128 * patch_size * patch_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whPmSt6ZzbUf"
      },
      "outputs": [],
      "source": [
        "patch_size = 64\n",
        "class LocalDiscriminator64(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_patches=4):\n",
        "        super(LocalDiscriminator64, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_channels * 2, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*8*8,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.fc = nn.Linear(128 * patch_size * patch_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Unxe3-DMaACC"
      },
      "outputs": [],
      "source": [
        "num_channels=3\n",
        "def gan_loss(generator, discriminator, local_discriminator, original_images, corrupted_images, lambda_recon=100, lambda_global=100, lambda_local=100):\n",
        "    generator=generator.to(device)\n",
        "    discriminator=discriminator.to(device)\n",
        "    local_discriminator=local_discriminator.to(device)\n",
        "    original_images=original_images.to(device)\n",
        "    corrupted_images=corrupted_images.to(device)\n",
        "    # Forward pass through the generator\n",
        "    inpainted_images = generator(corrupted_images)\n",
        "    # Concatenate original and inpainted images along the channel dimension\n",
        "    combined_images = torch.cat((original_images, inpainted_images), dim=1)\n",
        "    combined_images=combined_images.to(device)\n",
        "    # Forward pass through the discriminator\n",
        "    discriminator_output = discriminator(combined_images)\n",
        "    # Target labels for the discriminator\n",
        "    real_labels = torch.ones_like(discriminator_output)\n",
        "    fake_labels = torch.zeros_like(discriminator_output)\n",
        "    # Reconstruction loss (L1 loss)\n",
        "    recon_loss = nn.functional.l1_loss(inpainted_images, original_images) * lambda_recon\n",
        "    # Global discriminator loss (binary cross-entropy)\n",
        "    global_discriminator_loss = F.binary_cross_entropy(discriminator_output, real_labels) * lambda_global\n",
        "    # Local discriminator loss (binary cross-entropy on local patches)\n",
        "    local_discriminator_loss = 0.0\n",
        "    patch_size = 64\n",
        "    for i in range(0, original_images.size(2) - patch_size + 1, patch_size):\n",
        "        for j in range(0, original_images.size(3) - patch_size + 1, patch_size):\n",
        "            # Extract local patches from original and inpainted images\n",
        "            original_patch = original_images[:, :, i:i+patch_size, j:j+patch_size]\n",
        "            inpainted_patch = inpainted_images[:, :, i:i+patch_size, j:j+patch_size]\n",
        "            # Concatenate patches along the channel dimension\n",
        "            combined_patch = torch.cat((original_patch, inpainted_patch), dim=1)\n",
        "            combined_patch = combined_patch.to(device)\n",
        "            # Forward pass through the local discriminator\n",
        "            discriminator_output_patch = local_discriminator(combined_patch)\n",
        "            # Compute local discriminator loss\n",
        "            local_discriminator_loss += F.binary_cross_entropy(discriminator_output_patch, real_labels) * lambda_local\n",
        "    # Total GAN loss\n",
        "    total_loss = recon_loss + global_discriminator_loss + local_discriminator_loss\n",
        "    return total_loss, recon_loss, global_discriminator_loss, local_discriminator_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUY_pIabaj1P"
      },
      "outputs": [],
      "source": [
        "num_epochs=50\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "local_discriminator = LocalDiscriminator64()\n",
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "local_discriminator = local_discriminator.to(device)\n",
        "lambda_recon = 10\n",
        "lambda_global = 1\n",
        "lambda_local = 1\n",
        "\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=0.00001, betas=(0.5, 0.999))\n",
        "optimizer_local_discriminator = torch.optim.Adam(local_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_generator.zero_grad()\n",
        "optimizer_discriminator.zero_grad()\n",
        "optimizer_local_discriminator.zero_grad()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (original_images, corrupted_images) in tqdm(enumerate(dataloader)):\n",
        "        total_loss, recon_loss, global_discriminator_loss, local_discriminator_loss = gan_loss(generator, discriminator, local_discriminator, original_images, corrupted_images, lambda_recon, lambda_global, lambda_local)\n",
        "        total_loss.backward()\n",
        "        optimizer_generator.step()\n",
        "        optimizer_discriminator.step()\n",
        "        optimizer_local_discriminator.step()\n",
        "        optimizer_generator.zero_grad()\n",
        "        optimizer_discriminator.zero_grad()\n",
        "        optimizer_local_discriminator.zero_grad()\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], recon Loss: {recon_loss.item():.6f}, GD Loss: {global_discriminator_loss.item():.6f}, Local Loss: {local_discriminator_loss:.6f}, Total Loss: {total_loss.item():.6f}')\n",
        "    if epoch%5==4:\n",
        "        torch.save(generator.state_dict(), '/content/drive/MyDrive/generator.pt')\n",
        "        torch.save(discriminator.state_dict(), '/content/drive/MyDrive/discriminator.pt')\n",
        "        torch.save(local_discriminator.state_dict(), '/content/drive/MyDrive/local_discriminator.pt')\n",
        "        print(f'Models saved successfully on epoch: {epoch+1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD_Mmp5hbWMj"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.ImageFolder('/content/Train',transform=transform)\n",
        "validation_dataset = datasets.ImageFolder('/content/validation',transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YanEaWchbWgh"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=5):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def resnet50():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZXrTFOLbWkx"
      },
      "outputs": [],
      "source": [
        "model = resnet50()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.005,betas=(0.9,0.999))\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZM84E8EbWpQ"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(validation_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    average_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "    if epoch%5==4:\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaPVuRz9bWss"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in validation_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        predictions = predictions.cpu()\n",
        "        all_predictions.extend(predictions.numpy())\n",
        "result = torch.tensor(all_predictions)\n",
        "\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
        "true_values = []\n",
        "for input, label in validation_loader:\n",
        "    true_values.extend(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vuhr6XxpIB5T",
        "outputId": "6f020917-c61f-4fe7-ab55-860184a9c6c1"
      },
      "outputs": [],
      "source": [
        "pred = np.array(result)\n",
        "val = np.array(true_values)\n",
        "accuracy = accuracy_score(val, pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "conf_matrix = confusion_matrix(val, pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
